
R version 4.4.0 (2024-04-24) -- "Puppy Cup"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # bisection function
> 
> # - recursive implementation of bisection algorithm
> bisection <- function(low, high, W, u, eta){
+   mid <- (low + high)/2
+   f.low <- rCESS(W, u, low, eta)
+   f.mid <- rCESS(W, u, mid, eta)
+   f.high <- rCESS(W, u, high, eta)
+   
+   if(f.low * f.high > 0)
+     stop('Invalid endpoint for bisection.')
+   
+   try({if( low >= high )
+     stop('bisection overlap')
+     
+     if((abs(f.mid) < 1e-10)||((high - low)/2<1e-10))
+       return(mid)
+     if((f.low * f.mid) < 0)
+       return(bisection(low, mid, W, u, eta))
+     if((f.high * f.mid) < 0)
+       return(bisection(mid, high, W, u, eta))
+   })
+   
+   stop('bisection flawed')
+ }
> 
> # log-sum-exponential evaluation of log(sum(w)) (Avoids numerical underflow and overflow)
> logsum <- function(logw){
+   logmax = max(logw)
+   log(sum(exp(logw-logmax))) + logmax
+ }
> 
> rCESS <- function(W, u, a, eta) {
+   logw <- a * u          # weight update
+   exp(2*logsum(log(W)+logw) - logsum(log(W)+2*logw)) - eta
+ }
> 
> # effective sample size (Conditional effective sample size)
> rESS <- function(logW){
+   K <- length(logW) # number of particles
+   logWmax <- max(logW)
+   logRESS <- -(2*logWmax + log(sum(exp(2*logW-2*logWmax)))) - log(K)
+   return(exp(logRESS))
+ } 
> 
> systematic_resample <- function(W){
+   K <- length(W)
+   U <- runif(1,0,1/K) + 0:(K-1)/K # these evenly spaced points determine which weights are selected
+   W.sum <- cumsum(W)
+   N1 <- rep(NA,K)
+   j <- 1
+   
+   # re-sampling loop
+   for( i in 1:K )
+   {
+     found = F
+     while( !found )
+     {
+       if( U[i]>W.sum[j] )
+         j <- j+1
+       else
+         found = T
+     }
+     N1[i] <- j # (jth particle is selected as the ith re-sampled particle)
+   }
+   return( N1 ) # N1 is an array containing indices of the re-sampled particles.
+ }
> 
> # ASMC function ----------------------------------------------------------------
> 
> #' Annealed Sequential Monte Carlo
> #'
> #' Implementation of the annealed sequential Monte Carlo algorithm with adaptively determined annealing scheme
> #' @param K Number of particles
> #' @param theta_cov Either a proposal covariance matrix or a string "adaptive" to use an adaptive proposal scheme
> #' @param tuning_param List of tuning parameters for the ASMC algorithm: 1) eps - re-sampling threshold,
> #'  2) eta - target rCESS for adaptive re-sampling scheme, 3) alpha - annealing scheme. Only supply one of eta or alpha, not both.
> #' @param init Initialization function for the parameters
> #' @param data Matrix of data used in the likelihood
> #' @param likelihood Function to evaluate log-likelihood. Takes data and parameters as arguments.
> #' @param prior Function to evaluate the log-prior. Takes parameters as arguments.
> #' @param reference Function to evaluate the log-reference distribution. Takes parameters as arguments.
> #' @returns List of particles, weights, marginal log-likelihood, and effective sample size
> #' @export
> #' @examples
> #' 
> asmc <- function(K, theta_cov, tuning_param, init, data, likelihood, prior, reference){
+   
+   # initialize storage list
+   alpha <- list() # a list for tempering parameters
+   ESS <- list()   # a list for ESS
+   logZ <- list()  # list for log-normalizing constant
+   particles <- list()
+   W <- list()
+   
+   # check if adaptive annealing scheme is being used
+   adaptive_alpha <- "eta" %in% names(tuning_param) 
+   
+   r <- 1                
+   logZ[[r]] <- 0 # log-normalizing constant for the 1st iteration is set to 0.
+   
+   # initialize the annealing parameter 
+   if(adaptive_alpha){
+     alpha[[r]] <- 0    # tempering parameters
+     alphaDiff <- 0     # difference b/w two successive tempering parameters
+   } else{
+     alpha <- as.list(tuning_param$alpha)
+   }
+   
+   particles[[r]] <- lapply(1:K, function(k) list(theta = init(), accept = F))
+ 
+   weighted_cov <- diag(d)
+   W[[r]] <- rep(1/K,K)
+   logW <- log(W[[r]])
+   
+   # un-normalized weights
+   w <- rep(1,K) # initially all the un-normalized weights are set to 1.
+   logw <- rep(0,K)
+   
+   # begin iterative process
+   while( alpha[[r]]<1 )   # repeat this step if the tempering parameter is less than 1
+   {
+     cat("iteration:",r,"\n") # prints the current iteration number
+     r <- r+1  # increment iteration
+     # evaluate the log-likelihood for updating alpha
+     u <- rep(0, K)   # incremental log-importance weights
+     
+     # evaluate the log-likelihood for each particle using current data and previous theta.
+     u <- sapply(1:K, function(k){
+       logL <- likelihood(data, particles[[r-1]][[k]]$theta)
+       return(logL)
+     })
+     
+     if(adaptive_alpha){
+       alphaDiff <- bisection( 0, 1,  W[[r-1]], u, tuning_param$eta)
+       alpha[[r]] <- alpha[[r-1]] + alphaDiff
+     } else{
+       alphaDiff <- alpha[[r]] - alpha[[r-1]]
+     }
+     
+     cat("annealing parameter:",alpha[[r]],"\n") 
+     
+     if( alpha[[r]]>1 ){
+       alpha[[r]] <- 1
+       alphaDiff <- 1-alpha[[r-1]] # alphaDiff is recalculated
+     }
+     
+     # Updated MCMCmove Function
+     MCMCmove <- function(particle, theta_cov){
+       # Adaptive Metropolis-Hastings
+       MH <- function(theta_old, data, likelihood, prior, alpha){
+         theta_proposals <- matrix(NA, nrow = J, ncol = length(theta_old))
+         
+         for (j in 1:J) {
+           if(r <= 2 * d){
+             theta_cov <- 0.1^2 * diag(d)/d
+             theta_proposals[j, ] <- rtmvnorm(1, theta_old, theta_cov, lower = rep(0, d))
+           }
+           else{
+             if(runif(1) < b){
+               theta_cov <- 2.38^2 * weighted_cov / d
+               theta_proposals[j, ] <- rtmvnorm(1, theta_old, theta_cov, lower = rep(0, d))
+             }
+             else{
+               theta_cov <- 0.1^2 * diag(d) / d
+               theta_proposals[j, ] <- rtmvnorm(1, theta_old, theta_cov, lower = rep(0, d))
+             }
+           }
+         }
+         
+         # cat("tp", theta_proposals, "\n")
+         # Compute likelihoods and priors for all proposals
+         likelihoods <- alpha * apply(theta_proposals, 1, likelihood, data = data)
+         priors <- apply(theta_proposals, 1, prior)
+         
+         # Compute weights (unnormalized probabilities) for all proposals
+         logwyx <- likelihoods + priors
+         
+         #cat("lwyx", logwyx, "\n") 
+         # Normalize weights to get probabilities
+         probabilities <- exp((logwyx) - log(sum(exp(logwyx))))
+         
+         # Step 2: Select Y from the proposals based on the computed probabilities
+         index <- sample(1:J, size = 1, prob = probabilities)
+         theta_new <- theta_proposals[index, ]
+         
+         # Generate k-1 new proposals from T(theta_new, .) and compute weights for the new set
+         new_proposals <- matrix(NA, nrow = J - 1, ncol = length(theta_old))
+         
+         for (j in 1:(J - 1)) {
+           if(r <= 2 * d){
+             theta_cov <- 0.1^2 * diag(d)/d
+             new_proposals[j, ] <- rtmvnorm(1, theta_new, theta_cov, lower = rep(0, d)) 
+               #rtmvnorm(1, theta_new, theta_cov, lower = rep(0, d))
+           }
+           else{
+             if(runif(1) < b){
+               theta_cov <- 2.38^2 * weighted_cov / d
+               new_proposals[j, ] <- rtmvnorm(1, theta_new, theta_cov, lower = rep(0, d))  
+                 #rtmvnorm(1, theta_new, theta_cov, lower = rep(0, d))
+             }
+             else{
+               theta_cov <- 0.1^2 * diag(d)/d
+               new_proposals[j, ] <- rtmvnorm(1, theta_new, theta_cov, lower = rep(0, d)) 
+                 #rtmvnorm(1, theta_new, theta_cov, lower = rep(0, d))
+             }
+           }
+         }
+         
+         New_proposals <- rbind(new_proposals, theta_old)
+         new_likelihoods <- alpha * apply(New_proposals, 1, likelihood, data = data)
+         new_priors <- apply(New_proposals, 1, prior)
+         
+         logwxy <- new_likelihoods + new_priors 
+         
+         # Step 3: Accept or reject Y based on the acceptance ratio
+         ratio <- log(sum(exp(logwyx)))- log(sum(exp(logwxy)))
+         
+         # accept/reject step
+         if (log(runif(1, 0, 1)) < ratio){
+           return(list(theta = theta_new, accept = T))
+         } else{
+           return(list(theta = theta_old, accept = F))
+         }
+       }
+       # Calls the MH function
+       MH(particle$theta, data, likelihood, prior, alpha[[r]])
+     }   
+     
+     particles[[r]] <- lapply(particles[[r-1]],  MCMCmove, theta_cov)
+     
+     ## Converting the particles[[r]] list to a matrix with K rows and d columns
+     theta_values_r_1 <- lapply(particles[[r - 1]], function(particle) particle$theta)
+     theta_matrix_r_1 <- do.call(rbind, theta_values_r_1)
+     
+     # compute the ESS
+     log_incremental_w <- alphaDiff * u
+     logw <- log_incremental_w + logw  # log un-normalized weights are updated by adding incremental
+     # weight to the previous log un-normalized weight
+     logmax <- max(logw) # find the maximum of the updated log un-normalized weights
+     logZ[[r]] <- logZ[[r-1]] + logsum(log_incremental_w + logW)  # update the log normalizing constant 
+     W[[r]] <- exp(logw-logmax)/sum(exp(logw-logmax))   # normalized weights
+     logW <- log(W[[r]]) # log normalized weights
+     
+     ESS[[r]] <- rESS(logW) # Calculate the ESS 
+     
+     # re-sample if ESS below threshold
+     if( ESS[[r]]<tuning_param$eps )
+     {
+       cat("Resample: ESS=", ESS[[r]], '\n')
+       ancestors <- systematic_resample( W[[r]] )
+       particles[[r]] <-  particles[[r]][ancestors] #particles are then updated by selecting the particles 
+       # corresponding to the ancestors indices. This step effectively replaces particles with low weights 
+       # with duplicates of particles with higher weights
+       W[[r]] <- rep(1/K,K) # resets the normalized weights
+       logW <- log(W[[r]]) # re-calculate the logarithm of the normalized weights
+       w <- rep(1,K) # un-normalized weights are reset to 1
+       logw <- rep(0,K) # logarithm of un-normalized weights
+     }
+     
+     # Calculate the weighted covariance matrix
+     cov_weighted_l <- cov.wt(theta_matrix_r_1, W[[r - 1]])
+     weighted_cov <- cov_weighted_l[[1]]
+   }
+   
+   # these particles are the primary output of the algorithm. They represent the distribution of interest.
+   # logZ is used to calculate the marginal likelihood
+   output <- list(particles = particles,
+                  alpha = alpha,
+                  ESS = ESS,
+                  logZ = logZ,
+                  W = W)
+   return(output)
+ }
> 
> set.seed(444)
> 
> # Solving I(t) DE trajectory using ODE solver
> library(deSolve)
> library(mvtnorm)
> library(MASS)
> library(coda)
> library(MCMCpack)
##
## Markov Chain Monte Carlo Package (MCMCpack)
## Copyright (C) 2003-2024 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park
##
## Support provided by the U.S. National Science Foundation
## (Grants SES-0350646 and SES-0350613)
##
> library(truncnorm)
> library(tmvtnorm)
Loading required package: Matrix
Loading required package: stats4
Loading required package: gmm
Loading required package: sandwich
> 
> # initial (state) values for SIR model
> N <- 1000
> x0 <- c(S = N-6, I = 6, R = 0)
> 
> # vector of time steps
> times <- 0:59
> 
> # vector of parameters used in SIR model
> params <- c(beta = 0.2976, gamma = 1/8.5)
> 
> SIR <- function(t, x, params) {
+   with(as.list(c(params, x)), {
+     dS <- -beta * S * I / N 
+     dI <- beta * S * I / N - gamma * I
+     dR <- gamma * I 
+     list(c(dS, dI, dR))
+   })
+ }
> 
> r1 <-  rk4(x0, times, SIR, params)
> 
> I_values <- r1[, "I"]
> 
> mu_t <- I_values
> h <- 100
> data <- rnbinom(60, size = h, mu = mu_t)
> 
> data_table <- data.frame(Time = times, I_Values = round(I_values),Y = data)
> 
> # Define the Likelihood
> likelihood <- function(data,theta) {
+   beta <- theta[1] # transmission rate
+   gamma <- theta[2] # recovery rate
+   phi <- theta[3] # dispersion parameter of the negative binomial model
+   S0 <- theta[4] # initial number of susceptible individuals
+   I0 <- theta[5] # initial number of infected individuals
+   
+   params2 <- c(beta, gamma)
+   
+   SIR_model <- function(t, x, params2) {
+     with(as.list(c(params2, x)), {
+       dS <- -beta * S * I / N 
+       dI <- beta * S * I / N - gamma * I
+       dR <- gamma * I 
+       list(c(dS, dI, dR))
+     })
+   }
+   
+   x_0 <- c(S = S0, I = I0, R = N - (I0 + S0))
+   out <-  rk4(x_0, times, SIR_model, params2)
+   
+   I_values <- out[, "I"]
+   mu_t = I_values
+   #Loglikelihood <- 0
+   
+   # Calculate the log-likelihood
+   Loglikelihood <- sum(log(dnbinom(data, size = phi, mu = mu_t)))
+   return(Loglikelihood)
+ }
> 
> prior <- function(theta){
+   beta <- theta[1] # transmission rate
+   gamma <- theta[2] # recovery rate
+   phi <- theta[3] # dispersion parameter of the negative binomial model
+   S0 <- theta[4] # initial number of susceptible individuals
+   I0 <- theta[5] # initial number of infected individuals 
+   
+   log_prior_beta <- dlnorm(beta, log(0.2976), 0.2, log = TRUE)  
+   log_prior_gamma <- dlnorm(gamma, log(1/8.5), 0.2, log = TRUE)
+   log_prior_phi <- dgamma(phi, shape = 20000, rate = 200, log = TRUE)
+   log_prior_S0 <- dnorm(S0, 994, 1, log = TRUE)
+   log_prior_I0 <- dnorm(I0, 6, 1, log = TRUE)
+   
+   return(log_prior_beta + log_prior_gamma + log_prior_phi + log_prior_S0 + log_prior_I0)
+ }
> 
> reference <- prior
> 
> J <- 5
> K <- 1000
> d <- 5
> init <- function(){c(rlnorm(1,log(0.2976), 0.2),
+                      rlnorm(1,log(1/8.5), 0.2), 
+                      rgamma(1, 20000,rate = 200),
+                      rnorm(1, 994, 1),
+                      rnorm(1, 6, 1))}
>                      
> b <- 0.95
> tuning_param <- list(eps = 0.5, eta = 0.99)
> asmc_output <- asmc(K, theta_cov, tuning_param, init, data, likelihood, prior, reference)
iteration: 1 
annealing parameter: 0.0001260166 
iteration: 2 
annealing parameter: 0.0002992529 
iteration: 3 
annealing parameter: 0.0005044081 
iteration: 4 
annealing parameter: 0.0007739788 
iteration: 5 
annealing parameter: 0.001094775 
iteration: 6 
annealing parameter: 0.001451454 
iteration: 7 
annealing parameter: 0.001855662 
iteration: 8 
annealing parameter: 0.002339332 
iteration: 9 
annealing parameter: 0.002894497 
iteration: 10 
annealing parameter: 0.003514783 
iteration: 11 
annealing parameter: 0.004174313 
iteration: 12 
annealing parameter: 0.004954965 
iteration: 13 
annealing parameter: 0.00583364 
iteration: 14 
annealing parameter: 0.006821049 
iteration: 15 
annealing parameter: 0.007853829 
iteration: 16 
annealing parameter: 0.009023765 
iteration: 17 
annealing parameter: 0.01033768 
iteration: 18 
annealing parameter: 0.01178368 
iteration: 19 
annealing parameter: 0.01348078 
iteration: 20 
annealing parameter: 0.01529123 
iteration: 21 
annealing parameter: 0.01736694 
iteration: 22 
annealing parameter: 0.01971655 
iteration: 23 
annealing parameter: 0.02221936 
iteration: 24 
annealing parameter: 0.02501248 
iteration: 25 
annealing parameter: 0.02806085 
iteration: 26 
annealing parameter: 0.03133274 
iteration: 27 
annealing parameter: 0.03492809 
Resample: ESS= 0.4913654 
iteration: 28 
annealing parameter: 0.03876494 
iteration: 29 
annealing parameter: 0.04318206 
iteration: 30 
annealing parameter: 0.04815404 
iteration: 31 
annealing parameter: 0.0538321 
iteration: 32 
annealing parameter: 0.05998347 
iteration: 33 
annealing parameter: 0.06689369 
iteration: 34 
annealing parameter: 0.0745925 
iteration: 35 
annealing parameter: 0.08313468 
iteration: 36 
annealing parameter: 0.09251552 
iteration: 37 
annealing parameter: 0.1034495 
iteration: 38 
annealing parameter: 0.1148388 
iteration: 39 
annealing parameter: 0.1274902 
iteration: 40 
annealing parameter: 0.1412915 
iteration: 41 
annealing parameter: 0.156758 
iteration: 42 
annealing parameter: 0.1733103 
iteration: 43 
annealing parameter: 0.1906768 
iteration: 44 
annealing parameter: 0.2091521 
iteration: 45 
annealing parameter: 0.2297401 
iteration: 46 
annealing parameter: 0.2536448 
iteration: 47 
annealing parameter: 0.279282 
iteration: 48 
annealing parameter: 0.3068444 
iteration: 49 
annealing parameter: 0.3367945 
iteration: 50 
annealing parameter: 0.3679713 
Resample: ESS= 0.4844058 
iteration: 51 
annealing parameter: 0.4046859 
iteration: 52 
annealing parameter: 0.4432418 
iteration: 53 
annealing parameter: 0.4841539 
iteration: 54 
annealing parameter: 0.5286198 
iteration: 55 
annealing parameter: 0.5779884 
iteration: 56 
annealing parameter: 0.6325468 
iteration: 57 
annealing parameter: 0.6942621 
iteration: 58 
annealing parameter: 0.7610872 
iteration: 59 
annealing parameter: 0.8311023 
iteration: 60 
annealing parameter: 0.9088283 
iteration: 61 
annealing parameter: 0.9951999 
iteration: 62 
annealing parameter: 1.08487 
> R1 <- length(asmc_output$particles)
> 
> theta_R1 <- t(sapply(asmc_output$particles[[R1]], `[[`, "theta"))
> 
> W_R1 <- asmc_output$W[[R1]]
> 
> # post-run resampling
> resampled_indices <- sample(x = 1:K, size = K, replace = TRUE, 
+                             prob = W_R1)
> 
> resampled_particles <- theta_R1[resampled_indices,]
> 
> # Estimation of the parameters
> 
> mean_theta <- colMeans(resampled_particles)
> 
> # Initialize a matrix to store the credible intervals
> credible_intervals <- matrix(nrow = 2, ncol = ncol(resampled_particles))
> rownames(credible_intervals) <- c("Lower", "Upper")
> colnames(credible_intervals) <- colnames(resampled_particles)
> 
> # Calculate 95% credible intervals for each parameter
> for(i in 1:ncol(resampled_particles)) {
+   credible_intervals[, i] <- quantile(resampled_particles[, i], probs = c(0.025, 0.975))
+ }
> 
> Parameters <- c("Beta", "Gamma", "Phi", "S0", "I0")
> (result <- data.frame(Parameters, mean_theta, t(credible_intervals)))
  Parameters  mean_theta       Lower       Upper
1       Beta   0.3058600   0.2952088   0.3142241
2      Gamma   0.1194005   0.1155864   0.1230394
3        Phi  99.9545963  98.6597626 101.3234962
4         S0 994.0057285 992.1839094 996.2274569
5         I0   5.2574036   4.4537911   6.1739280
> 
> ASMC_AMTM_rsp <- unlist(resampled_particles)
> alpha_values <- unlist(asmc_output$alpha)
> ess_values <- unlist(asmc_output$ESS)
> W_values <-unlist(asmc_output$W)
> logZ_values <- unlist(asmc_output$logZ)
> 
> ASMC_AMTM_ESS <- data.frame(ESS = ess_values)
> ASMC_AMTM_alpha <- data.frame(alpha_val = alpha_values)
> ASMC_AMTM_W <- data.frame(W = W_values)
> ASMC_AMTM_logZ <- data.frame(logZ = logZ_values)
> 
> write.table(ASMC_AMTM_ESS, file = "/scratch/theniw8/ASMC_AMTM_SIR_1000_0.99_ESS.csv", sep = "," ,row.names = FALSE)
> write.table(ASMC_AMTM_alpha, file = "/scratch/theniw8/ASMC_AMTM_SIR_1000_0.99_alpha.csv", sep = "," ,row.names = FALSE)
> write.table(ASMC_AMTM_W, file = "/scratch/theniw8/ASMC_AMTM_SIR_1000_0.99_W.csv", sep = "," ,row.names = FALSE)
> write.table(ASMC_AMTM_logZ, file = "/scratch/theniw8/ASMC_AMTM_SIR_1000_0.99_logZ.csv", sep = "," ,row.names = FALSE)
> write.table(ASMC_AMTM_rsp, file = "/scratch/theniw8/ASMC_AMTM_SIR_1000_0.99_rsp.csv", sep = "," )
> 
